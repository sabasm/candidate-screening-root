version: '3.8'

services:
 llm:
   build:
     context: .
     dockerfile: infra/llm/Dockerfile
   volumes:
     - ./llm:/app/llm
     - ./data:/app/data
   ports:
     - "8000:8000"
   env_file:
     - .env
   environment:
     - PYTHONUNBUFFERED=1
     - LLM_API_KEY=${LLM_API_KEY:-}
     - LLM_MODEL=${LLM_MODEL:-gpt-4}
     - CACHE_TTL=${CACHE_TTL:-600}
     - USE_PROD=${USE_PROD:-false}
   networks:
     - app-network
   healthcheck:
     test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
     interval: 5s
     timeout: 5s
     retries: 5
     start_period: 2s

 frontend:
   build:
     context: .
     dockerfile: infra/app/Dockerfile
   volumes:
     - ./app:/app/app
     - /app/app/node_modules/
   ports:
     - "3000:3000"
   env_file:
     - .env
   environment:
     - NEXT_PUBLIC_LLM_API_URL=http://llm:8000
     - NEXT_PUBLIC_CLIENT_LLM_API_URL=http://localhost:8000
     - NEXT_USE_PROD=${NEXT_USE_PROD:-false}
   networks:
     - app-network
   depends_on:
     llm:
       condition: service_healthy
   healthcheck:
     test: ["CMD-SHELL", "wget -q --spider http://localhost:3000/api/health || exit 1"]
     interval: 5s
     timeout: 5s
     retries: 5
     start_period: 2s

networks:
 app-network:
   driver: bridge